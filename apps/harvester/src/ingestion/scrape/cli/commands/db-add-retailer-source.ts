import { readFileSync } from 'node:fs'
import { resolve } from 'node:path'
import { getRegisteredSitePluginManifest } from '../../registry.js'
import { safeJsonParse } from '../../kit/json.js'
import { validateScrapeConfig } from '../../kit/validate.js'
import { resolveRepoRoot } from '../paths.js'

type ScrapeConfigMergeMode = 'deep' | 'replace'

interface DbAddRetailerSourceArgs {
  siteId: string
  retailerName: string
  website: string
  sourceName: string
  sourceUrl: string
  scrapeConfigFile?: string
  scrapeConfigJson?: string
  scrapeConfigMerge?: ScrapeConfigMergeMode
  dryRun?: boolean
}

const SITE_ID_PATTERN = /^[a-z0-9_]+$/

function asSqlStringLiteral(value: string): string {
  return `'${value.replace(/'/g, "''")}'`
}

function asSqlJsonbLiteral(value: unknown): string {
  return `${asSqlStringLiteral(JSON.stringify(value))}::jsonb`
}

function normalizeAbsoluteHttpUrl(value: string, flag: string): string {
  let parsed: URL
  try {
    parsed = new URL(value)
  } catch {
    throw new Error(`${flag} must be a valid absolute URL`)
  }

  if (parsed.protocol !== 'https:' && parsed.protocol !== 'http:') {
    throw new Error(`${flag} must use http:// or https://`)
  }

  parsed.hash = ''
  return parsed.toString()
}

function readScrapeConfigFromInput(
  args: DbAddRetailerSourceArgs,
  repoRoot: string
): {
  config?: Record<string, unknown>
  unknownTopLevelKeys: string[]
} {
  if (args.scrapeConfigFile && args.scrapeConfigJson) {
    throw new Error('--scrape-config-file and --scrape-config-json are mutually exclusive')
  }

  if (!args.scrapeConfigFile && !args.scrapeConfigJson) {
    return { config: undefined, unknownTopLevelKeys: [] }
  }

  const payload = args.scrapeConfigFile
    ? readFileSync(resolve(repoRoot, args.scrapeConfigFile), 'utf8')
    : args.scrapeConfigJson!
  const parsed = safeJsonParse(payload)
  if (!parsed.ok) {
    throw new Error(`Invalid scrape config JSON: ${parsed.error}`)
  }

  const validation = validateScrapeConfig(parsed.value)
  if (!validation.ok) {
    throw new Error(validation.error)
  }

  return {
    config: validation.value,
    unknownTopLevelKeys: validation.unknownTopLevelKeys,
  }
}

function buildSqlScript(input: {
  siteId: string
  retailerName: string
  website: string
  sourceName: string
  sourceUrl: string
  sourceType: 'HTML' | 'JSON'
  mergeMode: ScrapeConfigMergeMode
  actor: string
  providedScrapeConfig?: Record<string, unknown>
  unknownTopLevelKeys: string[]
  dryRun: boolean
}): string {
  const providedScrapeConfigLiteral = input.providedScrapeConfig
    ? asSqlJsonbLiteral(input.providedScrapeConfig)
    : 'NULL::jsonb'
  const unknownTopLevelKeysLiteral = asSqlJsonbLiteral(input.unknownTopLevelKeys)
  const transactionEnd = input.dryRun ? 'ROLLBACK;' : 'COMMIT;'
  const modeLabel = input.dryRun ? 'dry-run' : 'apply'

  return [
    'BEGIN;',
    '',
    '-- Generated by pnpm scraper:db:add-retailer-source (text-only SQL mode)',
    `-- siteId: ${input.siteId}`,
    `-- mode: ${modeLabel}`,
    '-- Review before running against your chosen environment.',
    '',
    'DO $$',
    'DECLARE',
    `  v_site_id text := ${asSqlStringLiteral(input.siteId)};`,
    `  v_retailer_name text := ${asSqlStringLiteral(input.retailerName)};`,
    `  v_website text := ${asSqlStringLiteral(input.website)};`,
    `  v_source_name text := ${asSqlStringLiteral(input.sourceName)};`,
    `  v_source_url text := ${asSqlStringLiteral(input.sourceUrl)};`,
    `  v_source_type text := ${asSqlStringLiteral(input.sourceType)};`,
    `  v_merge_mode text := ${asSqlStringLiteral(input.mergeMode)};`,
    `  v_actor text := ${asSqlStringLiteral(input.actor)};`,
    `  v_provided_scrape_config jsonb := ${providedScrapeConfigLiteral};`,
    `  v_unknown_top_level_keys jsonb := ${unknownTopLevelKeysLiteral};`,
    '  v_retailer_id text;',
    '  v_source_id text;',
    '  v_source_match_count integer := 0;',
    '  v_old_retailer jsonb;',
    '  v_old_adapter jsonb;',
    '  v_old_source jsonb;',
    '  v_old_trust jsonb;',
    '  v_resolved_scrape_config jsonb;',
    'BEGIN',
    "  IF v_merge_mode NOT IN ('deep', 'replace') THEN",
    "    RAISE EXCEPTION 'Invalid scrape-config merge mode: %', v_merge_mode;",
    '  END IF;',
    '',
    '  CREATE OR REPLACE FUNCTION pg_temp.jsonb_deep_merge(a jsonb, b jsonb)',
    '  RETURNS jsonb',
    '  LANGUAGE sql',
    '  IMMUTABLE',
    '  AS $merge$',
    '    SELECT CASE',
    "      WHEN jsonb_typeof(a) <> 'object' OR jsonb_typeof(b) <> 'object' THEN b",
    '      ELSE (',
    '        SELECT jsonb_object_agg(key, value)',
    '        FROM (',
    '          SELECT',
    '            COALESCE(ka.key, kb.key) AS key,',
    '            CASE',
    '              WHEN ka.value IS NULL THEN kb.value',
    '              WHEN kb.value IS NULL THEN ka.value',
    "              WHEN jsonb_typeof(ka.value) = 'object' AND jsonb_typeof(kb.value) = 'object'",
    '                THEN pg_temp.jsonb_deep_merge(ka.value, kb.value)',
    '              ELSE kb.value',
    '            END AS value',
    '          FROM jsonb_each(a) ka',
    '          FULL OUTER JOIN jsonb_each(b) kb ON ka.key = kb.key',
    '        ) merged',
    '      )',
    '    END',
    '  $merge$;',
    '',
    '  SELECT to_jsonb(r) INTO v_old_retailer',
    '  FROM retailers r',
    '  WHERE r.website = v_website',
    '  LIMIT 1;',
    '',
    '  INSERT INTO retailers (name, website, "visibilityStatus")',
    "  VALUES (v_retailer_name, v_website, 'INELIGIBLE')",
    '  ON CONFLICT (website)',
    '  DO UPDATE SET name = EXCLUDED.name',
    '  RETURNING id INTO v_retailer_id;',
    '',
    '  SELECT to_jsonb(sas) INTO v_old_adapter',
    '  FROM scrape_adapter_status sas',
    '  WHERE sas."adapterId" = v_site_id',
    '  LIMIT 1;',
    '',
    '  INSERT INTO scrape_adapter_status ("adapterId", enabled)',
    '  VALUES (v_site_id, TRUE)',
    '  ON CONFLICT ("adapterId") DO NOTHING;',
    '',
    '  SELECT COUNT(*)::integer INTO v_source_match_count',
    '  FROM (',
    '    SELECT DISTINCT s.id',
    '    FROM sources s',
    '    WHERE s."adapterId" = v_site_id',
    '      OR (s."retailerId" = v_retailer_id AND s.url = v_source_url)',
    '      OR (s."retailerId" = v_retailer_id AND s.name = v_source_name AND s."adapterId" = v_site_id)',
    '      OR (s."retailerId" = v_retailer_id AND s.name = v_source_name AND s."adapterId" IS NULL)',
    '  ) source_candidates;',
    '',
    '  IF v_source_match_count > 1 THEN',
    "    RAISE EXCEPTION 'Ambiguous source match for site %, matched % rows', v_site_id, v_source_match_count;",
    '  END IF;',
    '',
    '  SELECT source_candidates.id INTO v_source_id',
    '  FROM (',
    '    SELECT DISTINCT s.id',
    '    FROM sources s',
    '    WHERE s."adapterId" = v_site_id',
    '      OR (s."retailerId" = v_retailer_id AND s.url = v_source_url)',
    '      OR (s."retailerId" = v_retailer_id AND s.name = v_source_name AND s."adapterId" = v_site_id)',
    '      OR (s."retailerId" = v_retailer_id AND s.name = v_source_name AND s."adapterId" IS NULL)',
    '    ORDER BY s.id',
    '    LIMIT 1',
    '  ) source_candidates;',
    '',
    '  IF v_source_id IS NOT NULL THEN',
    '    SELECT to_jsonb(s) INTO v_old_source',
    '    FROM sources s',
    '    WHERE s.id = v_source_id;',
    '',
    "    IF v_old_source ->> 'retailerId' IS DISTINCT FROM v_retailer_id THEN",
    "      RAISE EXCEPTION 'Matched source % belongs to another retailer', v_source_id;",
    '    END IF;',
    '',
    "    IF COALESCE(v_old_source ->> 'adapterId', '') <> ''",
    "      AND v_old_source ->> 'adapterId' <> v_site_id THEN",
    "      RAISE EXCEPTION 'Matched source % already uses adapter %', v_source_id, v_old_source ->> 'adapterId';",
    '    END IF;',
    '  END IF;',
    '',
    '  IF v_provided_scrape_config IS NOT NULL THEN',
    "    IF v_merge_mode = 'replace'",
    '      OR v_old_source IS NULL',
    "      OR v_old_source -> 'scrapeConfig' IS NULL",
    "      OR jsonb_typeof(v_old_source -> 'scrapeConfig') = 'null' THEN",
    '      v_resolved_scrape_config := v_provided_scrape_config;',
    '    ELSE',
    "      IF jsonb_typeof(v_old_source -> 'scrapeConfig') <> 'object' THEN",
    "        RAISE EXCEPTION 'Existing sources.scrapeConfig is not an object; deep merge is not safe';",
    '      END IF;',
    '      v_resolved_scrape_config := pg_temp.jsonb_deep_merge(',
    "        v_old_source -> 'scrapeConfig',",
    '        v_provided_scrape_config',
    '      );',
    '    END IF;',
    '  END IF;',
    '',
    '  IF v_source_id IS NULL THEN',
    '    INSERT INTO sources (',
    '      name,',
    '      url,',
    '      type,',
    '      enabled,',
    '      "retailerId",',
    '      "adapterId",',
    '      "scrapeEnabled",',
    '      "robotsCompliant",',
    '      "scrapeConfig"',
    '    )',
    '    VALUES (',
    '      v_source_name,',
    '      v_source_url,',
    '      v_source_type::"SourceType",',
    '      TRUE,',
    '      v_retailer_id,',
    '      v_site_id,',
    '      FALSE,',
    '      TRUE,',
    '      v_resolved_scrape_config',
    '    )',
    '    RETURNING id INTO v_source_id;',
    '  ELSE',
    '    UPDATE sources',
    '    SET',
    '      name = v_source_name,',
    '      url = v_source_url,',
    '      type = v_source_type::"SourceType",',
    '      "adapterId" = v_site_id,',
    '      "scrapeConfig" = COALESCE(v_resolved_scrape_config, "scrapeConfig")',
    '    WHERE id = v_source_id;',
    '  END IF;',
    '',
    '  SELECT to_jsonb(stc) INTO v_old_trust',
    '  FROM source_trust_config stc',
    '  WHERE stc."sourceId" = v_source_id',
    '  LIMIT 1;',
    '',
    '  INSERT INTO source_trust_config ("sourceId", "upcTrusted")',
    '  VALUES (v_source_id, FALSE)',
    '  ON CONFLICT ("sourceId") DO NOTHING;',
    '',
    '  INSERT INTO admin_audit_logs (',
    '    "adminUserId",',
    '    action,',
    '    resource,',
    '    "resourceId",',
    '    "oldValue",',
    '    "newValue"',
    '  )',
    '  VALUES (',
    '    v_actor,',
    "    'SCRAPER_DB_ADD_RETAILER_SOURCE',",
    "    'scraper_source_onboarding',",
    '    v_source_id,',
    '    jsonb_build_object(',
    "      'retailer', v_old_retailer,",
    "      'scrapeAdapterStatus', v_old_adapter,",
    "      'source', v_old_source,",
    "      'sourceTrustConfig', v_old_trust",
    '    )::json,',
    '    jsonb_build_object(',
    "      'siteId', v_site_id,",
    "      'retailer', (SELECT to_jsonb(r) FROM retailers r WHERE r.id = v_retailer_id),",
    "      'scrapeAdapterStatus', (SELECT to_jsonb(sas) FROM scrape_adapter_status sas WHERE sas.\"adapterId\" = v_site_id),",
    "      'source', (SELECT to_jsonb(s) FROM sources s WHERE s.id = v_source_id),",
    "      'sourceTrustConfig', (SELECT to_jsonb(stc) FROM source_trust_config stc WHERE stc.\"sourceId\" = v_source_id),",
    "      'scrapeConfigMerge', v_merge_mode,",
    "      'unknownScrapeConfigTopLevelKeys', v_unknown_top_level_keys",
    '    )::json',
    '  );',
    '',
    "  RAISE NOTICE 'Prepared onboarding for site %, retailer %, source %', v_site_id, v_retailer_id, v_source_id;",
    'END',
    '$$;',
    '',
    transactionEnd,
    '',
  ].join('\n')
}

export async function runDbAddRetailerSourceCommand(
  args: DbAddRetailerSourceArgs
): Promise<number> {
  const repoRoot = resolveRepoRoot()
  const missing = []
  if (!args.siteId) missing.push('--site-id')
  if (!args.retailerName) missing.push('--retailer-name')
  if (!args.website) missing.push('--website')
  if (!args.sourceName) missing.push('--source-name')
  if (!args.sourceUrl) missing.push('--source-url')

  if (missing.length > 0) {
    console.error(`Missing required arguments: ${missing.join(', ')}`)
    return 2
  }

  if (!SITE_ID_PATTERN.test(args.siteId)) {
    console.error('siteId must match /^[a-z0-9_]+$/')
    return 2
  }

  const manifest = getRegisteredSitePluginManifest(args.siteId)
  if (!manifest) {
    console.error(`Unknown site plugin id '${args.siteId}'. Run scraper:add first or verify registry wiring.`)
    return 2
  }

  const mergeMode = args.scrapeConfigMerge ?? 'deep'
  if (mergeMode !== 'deep' && mergeMode !== 'replace') {
    console.error("--scrape-config-merge must be either 'deep' or 'replace'")
    return 2
  }

  try {
    const website = normalizeAbsoluteHttpUrl(args.website, '--website')
    const sourceUrl = normalizeAbsoluteHttpUrl(args.sourceUrl, '--source-url')
    const { config: providedScrapeConfig, unknownTopLevelKeys } = readScrapeConfigFromInput(
      args,
      repoRoot
    )
    const actor =
      process.env.SCRAPER_CLI_ACTOR?.trim() ||
      process.env.USER?.trim() ||
      process.env.USERNAME?.trim() ||
      'SYSTEM_SCRAPER_CLI'

    const sourceType = manifest.mode === 'json' ? 'JSON' : 'HTML'
    const sqlScript = buildSqlScript({
      siteId: args.siteId,
      retailerName: args.retailerName,
      website,
      sourceName: args.sourceName,
      sourceUrl,
      sourceType,
      mergeMode,
      actor,
      providedScrapeConfig,
      unknownTopLevelKeys,
      dryRun: args.dryRun === true,
    })

    console.log(sqlScript)

    return 0
  } catch (error) {
    console.error(error instanceof Error ? error.message : String(error))
    return 2
  }
}

apiVersion: 1

groups:
  # ──────────────────────────────────────────────
  # IronScout Critical (eval interval: 1m)
  # ──────────────────────────────────────────────
  - orgId: 1
    name: IronScout Critical
    folder: IronScout
    interval: 1m
    rules:
      # Rule 1: Fatal Error
      - uid: ironscout-fatal-error
        title: Fatal Error
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "\"level\":\"fatal\""[5m]) > 0
              instant: true
              refId: A
        for: 0s
        noDataState: OK
        execErrState: Error
        labels:
          severity: critical
          alertname: ironscout-fatal-error
          owner: ops
        annotations:
          summary: Fatal error detected in IronScout logs
          description: A fatal-level log event was emitted by one of the IronScout services. This typically indicates an unrecoverable error that crashed or will crash the process.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22%5C%5C%5C%22level%5C%5C%5C%22:%5C%5C%5C%22fatal%5C%5C%5C%22%5C%22%22%7D%5D%7D
          runbook: |
            1. Check Grafana Explore for the fatal log line — identify service and stack trace.
            2. Check if service restarted (correlate with API/Harvester restart events).
            3. Escalation: if fatal persists after restart, page on-call.

      # Rule 2: API Restart Storm
      - uid: ironscout-api-restart-storm
        title: API Restart Storm
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 600
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "API server started"[10m]) >= 3
              instant: true
              refId: A
        for: 0s
        noDataState: OK
        execErrState: Error
        labels:
          severity: critical
          alertname: ironscout-api-restart-storm
          owner: ops
        annotations:
          summary: API restarted 3+ times in 10 minutes
          description: The API service has restarted 3 or more times in a 10-minute window. This indicates a crash loop. Each restart is detected by the "API server started" log message (exactly 1 per boot).
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22API%20server%20started%5C%22%22%7D%5D%7D
          runbook: |
            1. Check Render dashboard for API service deploy or crash events.
            2. Check Grafana for error logs immediately before each restart.
            3. Escalation: if crash-looping with no deploy, check for OOM or unhandled exception.

      # Rule 3: DB Connectivity
      - uid: ironscout-db-connectivity
        title: DB Connectivity
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "PrismaClientInitializationError|Can.t reach database|ECONNREFUSED|ENOTFOUND|Connection terminated|remaining connection slots|too many clients"[5m]) > 0
              instant: true
              refId: A
        for: 2m
        noDataState: OK
        execErrState: Error
        labels:
          severity: critical
          alertname: ironscout-db-connectivity
          owner: ops
        annotations:
          summary: Database connectivity failure detected
          description: Database connectivity errors detected — covers Prisma initialization failures, DNS resolution failures, TCP connection failures, mid-query disconnects, and connection pool saturation.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22PrismaClientInitializationError%7CCan.t%20reach%20database%7CECONNREFUSED%7CENOTFOUND%7CConnection%20terminated%7Cremaining%20connection%20slots%7Ctoo%20many%20clients%5C%22%22%7D%5D%7D
          runbook: |
            1. Check Render PostgreSQL dashboard for status/maintenance.
            2. Check for "remaining connection slots" or "too many clients" (pool saturation).
            3. Restart API only after DB confirmed healthy; if DB is healthy, check for connection leak in recent deploys.

      # Rule 4: Harvester Restart Storm
      - uid: ironscout-harvester-restart-storm
        title: Harvester Restart Storm
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 600
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "Scheduler settings loaded"[10m]) >= 3
              instant: true
              refId: A
        for: 0s
        noDataState: OK
        execErrState: Error
        labels:
          severity: critical
          alertname: ironscout-harvester-restart-storm
          owner: ops
        annotations:
          summary: Harvester restarted 3+ times in 10 minutes
          description: The harvester service has restarted 3 or more times in a 10-minute window. This indicates a crash loop. Rapid restarts risk ADR-001 scheduler singleton violations and BullMQ stalled job accumulation.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22Scheduler%20settings%20loaded%5C%22%22%7D%5D%7D
          runbook: |
            1. Check Render dashboard for harvester service crashes.
            2. Check for Redis/DB connectivity errors preceding each restart.
            3. Escalation: disable scheduler via Admin UI > Settings > Danger Zone > toggle "Main Harvester Scheduler" to Disable to prevent ADR-001 scheduler overlap violations.

      # Rule 5: Log Ingestion Stalled
      - uid: ironscout-log-ingestion-stalled
        title: Log Ingestion Stalled
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 600
              to: 0
            model:
              expr: count_over_time({source="render"}[10m]) == 0
              instant: true
              refId: A
        for: 5m
        noDataState: Alerting
        execErrState: Alerting
        labels:
          severity: critical
          alertname: ironscout-log-ingestion-stalled
          owner: ops
        annotations:
          summary: No logs arriving in Loki from Render services
          description: Zero log lines received from {source="render"} in the last 10 minutes. This means the Render -> Alloy -> Loki pipeline is broken. All other threshold-based alert rules will silently evaluate against empty data and report OK.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%22%7D%5D%7D
          runbook: |
            1. SSH to logging host (10.10.9.28) — check Alloy process status (systemctl status alloy) and syslog receiver.
            2. Check Loki health endpoint (http://10.10.9.28:3100/ready, /metrics).
            3. Check Render syslog drain configuration for changes (Render Dashboard > Environment > Syslog Drain).

  # ──────────────────────────────────────────────
  # IronScout Warning (eval interval: 2m)
  # ──────────────────────────────────────────────
  - orgId: 1
    name: IronScout Warning
    folder: IronScout
    interval: 2m
    rules:
      # Rule 6: Error Rate Spike
      - uid: ironscout-error-rate-spike
        title: Error Rate Spike
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "\"level\":\"error\""[5m]) > 20
              instant: true
              refId: A
        for: 5m
        noDataState: OK
        execErrState: Error
        labels:
          severity: warning
          alertname: ironscout-error-rate-spike
          owner: ops
        annotations:
          summary: Error rate exceeds 20 errors in 5 minutes
          description: More than 20 error-level log events in a 5-minute window, sustained for 5 minutes. This threshold is calibrated for major incidents only. Use /check-logs for faster detection of minor degradation.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22%5C%5C%5C%22level%5C%5C%5C%22:%5C%5C%5C%22error%5C%5C%5C%22%5C%22%22%7D%5D%7D
          runbook: |
            1. Check Grafana Explore — filter by "level":"error" and group by error message.
            2. Identify if errors are from one service or spread across all.
            3. Escalation: if localized, check recent deploys to that service.

      # Rule 7: Prisma Errors
      - uid: ironscout-prisma-errors
        title: Prisma Errors
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "PrismaClientValidationError|PrismaClientKnownRequestError"[5m]) >= 5
              instant: true
              refId: A
        for: 5m
        noDataState: OK
        execErrState: Error
        labels:
          severity: warning
          alertname: ironscout-prisma-errors
          owner: ops
        annotations:
          summary: Sustained Prisma errors (5+ in 5 minutes)
          description: PrismaClientValidationError or PrismaClientKnownRequestError occurring at a sustained rate. ValidationError indicates code bugs; KnownRequestError indicates data/constraint issues.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22PrismaClientValidationError%7CPrismaClientKnownRequestError%5C%22%22%7D%5D%7D
          runbook: |
            1. Check error messages — ValidationError = code bug; KnownRequestError = data/constraint issue.
            2. Correlate with recent deploys.
            3. Escalation: if sustained and not deploy-related, check for schema drift.

      # Rule 8: Ingest Run Failures
      - uid: ironscout-ingest-run-failures
        title: Ingest Run Failures
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 1800
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "INGEST_RUN_SUMMARY" |~ "\"status\":\"FAILED\""[30m]) >= 3
              instant: true
              refId: A
        for: 5m
        noDataState: OK
        execErrState: Error
        labels:
          severity: warning
          alertname: ironscout-ingest-run-failures
          owner: ops
        annotations:
          summary: 3+ failed ingest runs in 30 minutes
          description: Multiple INGEST_RUN_SUMMARY events with status FAILED in a 30-minute window. Covers both AFFILIATE and RETAILER pipelines. This alert is the only Slack coverage for RETAILER pipeline failures.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22INGEST_RUN_SUMMARY%5C%22%20%7C~%20%5C%22%5C%5C%5C%22status%5C%5C%5C%22:%5C%5C%5C%22FAILED%5C%5C%5C%22%5C%22%22%7D%5D%7D
          runbook: |
            1. Check INGEST_RUN_SUMMARY logs for pipeline (AFFILIATE vs RETAILER) and error codes.
            2. For AFFILIATE: check if circuit breaker fired or feed was auto-disabled.
            3. Escalation: if multiple feeds failing, check upstream source availability.

      # Rule 9: Ingestion Stalled
      - uid: ironscout-ingestion-stalled
        title: Ingestion Stalled
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 21600
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "INGEST_RUN_SUMMARY"[6h]) == 0
              instant: true
              refId: A
        for: 30m
        noDataState: OK
        execErrState: Error
        labels:
          severity: warning
          alertname: ironscout-ingestion-stalled
          owner: ops
        annotations:
          summary: No ingest run summaries in 6 hours
          description: Zero INGEST_RUN_SUMMARY events in 6 hours. This could mean the harvester is down, the scheduler is disabled, or the harvester is crash-looping before producing summaries. Worst-case detection latency is ~6h30m. Use /check-logs 1h for faster verification during active triage.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22INGEST_RUN_SUMMARY%5C%22%22%7D%5D%7D
          runbook: |
            1. Check harvester service status on Render (running? restarting?).
            2. Check if scheduler is intentionally disabled via Admin Settings.
            3. If unintentional: check Redis connectivity and BullMQ queue status via Bull Board.

      # Rule 10: Redis Connectivity
      - uid: ironscout-redis-connectivity
        title: Redis Connectivity
        condition: A
        data:
          - refId: A
            datasourceUid: ${LOKI_DATASOURCE_UID}
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: count_over_time({source="render"} |~ "MaxRetriesPerRequestError|ECONNREFUSED.*6379|redis.*NOAUTH|READONLY.*redis|Connection is closed"[5m]) > 0
              instant: true
              refId: A
        for: 3m
        noDataState: OK
        execErrState: Error
        labels:
          severity: warning
          alertname: ironscout-redis-connectivity
          owner: ops
        annotations:
          summary: Redis connectivity errors detected
          description: Redis connectivity errors detected — covers max retries exhausted, connection refused, auth failures (NOAUTH), read-only replica errors, and closed connections. Does NOT cover queue backlog growth or stalled job accumulation (known blind spot).
          owner: ops
          dashboard_url: ${GRAFANA_URL}/explore?orgId=1&left=%7B%22queries%22:%5B%7B%22expr%22:%22%7Bsource%3D%5C%22render%5C%22%7D%20%7C~%20%5C%22MaxRetriesPerRequestError%7CECONNREFUSED.*6379%7Credis.*NOAUTH%7CREADONLY.*redis%7CConnection%20is%20closed%5C%22%22%7D%5D%7D
          runbook: |
            1. Check Render Redis dashboard for status/maintenance.
            2. After Redis recovers: check BullMQ queues via Bull Board for stalled/waiting backlog.
            3. Escalation: if Redis is healthy, check for NOAUTH (credential rotation) or READONLY (failover).

  # ──────────────────────────────────────────────
  # IronScout Watchdog (eval interval: 5m)
  # ──────────────────────────────────────────────
  - orgId: 1
    name: IronScout Watchdog
    folder: IronScout
    interval: 5m
    rules:
      # W1: Monitoring Heartbeat (dead man's switch)
      - uid: ironscout-watchdog
        title: Monitoring Heartbeat
        condition: A
        data:
          - refId: A
            datasourceUid: __expr__
            relativeTimeRange:
              from: 300
              to: 0
            model:
              type: math
              expression: "1"
              refId: A
        for: 0s
        noDataState: Alerting
        execErrState: Alerting
        labels:
          severity: warning
          alertname: ironscout-watchdog
          owner: ops
        annotations:
          summary: IronScout monitoring heartbeat (dead man's switch)
          description: This alert is intentionally always firing. Expect periodic heartbeat messages in Slack at the repeat_interval cadence (4h for warning severity). If Slack stops receiving this heartbeat, either Grafana cannot evaluate rules or cannot deliver notifications. The watchdog does not confirm Slack actually displays messages — verify heartbeat arrival manually.
          owner: ops
          dashboard_url: ${GRAFANA_URL}/alerting/list
          runbook: |
            1. Check Grafana service is running and alert evaluation is not paused.
            2. Test Slack contact point manually (Alerting > Contact points > Test).
            3. Check Grafana server logs for delivery errors.
